{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting, ensure that the kernel is set to 'PySpark'\n",
    "\n",
    "# To run our notebook on the EMR Cluster, we firstly need to create the SparkSession. \n",
    "# Since we're running this in the Pyspark Kernel, the SparkSession is automatically defined as 'spark'.\n",
    "# It will start the spark session as soon as you run a block of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that sparkSession has been started, we want to do a few configurations on our session in an attempt to\n",
    "# slightly optimize it's performance. \n",
    "# You may add/remove/change any configuration settings according to your needs/cluster configuration.\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 7500)\n",
    "spark.conf.set(\"spark.executor.memory\", '2g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally in python, we are able to install our packages using pip3 or some other package manager. Since we have\n",
    "# multiple machines in our cluster, we aren't able to use simple pip3 commands inside the command line to install \n",
    "# packages efficiently. \n",
    "# So, we can use the sparkContext to install our packages on our machines for us.\n",
    "\n",
    "# This can be done by first importing and creating the sparkContext.\n",
    "from pyspark.context import SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# And then using the install_pypi_package function to install the packages.\n",
    "# It uses PyPi package index, so make sure you get the package names from: https://pypi.org/\n",
    "sc.install_pypi_package(\"statsmodels\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"pyarrow==0.11.0\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have all of our packages installed using the install_pypi_package function, we can now import these\n",
    "# packages into our notebook.\n",
    "import numpy as np, pandas as pd\n",
    "import calendar\n",
    "import ast\n",
    "import datetime\n",
    "import logging\n",
    "import statsmodels.api as sm\n",
    "import pyarrow as pa\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notebook is now setup to run this example.\n",
    "\n",
    "# To get data from the Redshift database, we can use the spark session to read the data straight into a dataframe.\n",
    "# This can be done by using the following line for each table:\n",
    "# dataframe = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:redshift://host:port/database\").option(\"dbtable\", \"table name here\").option(\"user\", \"username here\").option(\"password\", \"password here\").load()\n",
    "# You can configure this line with your own parameters.\n",
    "\n",
    "# For our example, we can use these two lines to read the data from the fcst20.state_to_state table, and \n",
    "# the parameters from the fcst20.arima_calib table. (You still need to configure your own Username and password)\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:redshift://demo.3victorsaws.com:5439/demo\").option(\"dbtable\", \"fcst20.state_to_state\").option(\"user\", \"\").option(\"password\", \"\").load()\n",
    "params_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:redshift://demo.3victorsaws.com:5439/demo\").option(\"dbtable\", \"fcst20.arima_calib\").option(\"user\", \"\").option(\"password\", \"\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we can start to manipulate the data in the Spark dataframes, we need to declare a schema. This schema will be\n",
    "# used to tell PySpark how to automatically translate the Pandas dataframe that we return from our functions back into \n",
    "# a Spark Dataframe. The schema is the structure of the Pandas dataframe that we're return.\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "                     StructField('origin_state_code', StringType(), True),\n",
    "                     StructField('destination_state_code', StringType(), True),\n",
    "                     StructField('travel_month', StringType(), True),\n",
    "                     StructField('index', StringType(), True),\n",
    "                     StructField('mean', FloatType(), True),\n",
    "                     StructField('mean_se', FloatType(), True),  \n",
    "                     StructField('mean_ci_lower', FloatType(), True),\n",
    "                     StructField('mean_ci_upper', FloatType(), True)                     \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have declared what the returned dataframe's schema is, we can create our function. This function will be\n",
    "# used to run predictions on Grouped Data. Grouped Data, in this notebook is the data in our Spark Dataframe that has\n",
    "# been split up into smaller Pandas Dataframes depending on the values of origin_state_code, destination_state_code \n",
    "# and  month_state for each group of rows.\n",
    "# This means that the data used in each instance of this function will have the same values of each of these variables.\n",
    "\n",
    "# The function requires a decorator in order to be used with our spark dataframe. \n",
    "# The first value in the decorator indicates our schema/format for the outputted pandas dataframe.\n",
    "# The second value in the decorator tells Pyspark that the outputted/returned Pandas dataframe should be converted\n",
    "# back into a spark dataframe upon completion. Additionally it also tells PySpark that the function should be run\n",
    "# on data according to the groupBy() function ran before it. \n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def run_sarima_bystate(state_data):\n",
    "    # Since we have told pyspark that the state_data variable in this function should be grouped data,\n",
    "    # we can start to manipulate data. Additionally, as mentioned before, the grouped data is stored using Pandas\n",
    "    # whilst in this function.\n",
    "    \n",
    "    \n",
    "    # In order to be able to run our prediction on our data, we need to firstly format the Pandas Dataframe.\n",
    "    # This is done by firstly sorting the dataframe by the date column. (This is so it can be used as an index later on.\n",
    "    state_data = state_data.sort_values('sales_date', ascending=True) \n",
    " \n",
    "    # For simplicity and readability we can split the state_data dataframe, which contains a combination of our\n",
    "    # state_to_state table and params table, into the data needed for the prediction.\n",
    "    data=pd.DataFrame() \n",
    "    # Converting the 'date' column of state_data from string to date\n",
    "    data['date']           = pd.to_datetime(state_data['date']) \n",
    "    # Copying the 'total_searches' column from one df to the other.\n",
    "    data['total_searches'] = state_data['total_searches']\n",
    "    \n",
    "    # As mentioned before, the data inside the function is grouped, which means that we only have data which have \n",
    "    # the exact same values for origin_state_code, destination_state_code and month_start.\n",
    "    # This means that we can actually just take a singular value from each of the columns in the dataframe and use \n",
    "    # them as variables.\n",
    "    origin_state      = state_data['origin_state_code'][0] \n",
    "    destination_state = state_data['destination_state_code'][0]\n",
    "    # (Data is grouped by month_start, so it's grouped by month_end too)\n",
    "    month_end         = pd.to_datetime(state_data['month_end'])[0].strftime('%Y-%m-%d') \n",
    "    \n",
    "    # We want to be able to tell the Sarimax model where to learn and where to predict from.\n",
    "    # Get Yesterday's date as it will be the last day of data to for the model to learn from.\n",
    "    yesterday         = (datetime.datetime.now() + datetime.timedelta(-1)).strftime('%Y-%m-%d') \n",
    "    # Get today's date, as this will be the cut off as to where to start forecasting.\n",
    "    today             = datetime.datetime.now().strftime('%Y-%m-%d') \n",
    "    \n",
    "    # Sarimax needs a continuous column of dates, which each row containing data. So if there's data missing, it\n",
    "    # has the potential to cause our Sarimax model to fail.\n",
    "    # We can combat this by creating a column containing all of our desired dates, merging those dates to our data \n",
    "    # dataframe, and filling any empty values with the value of 0.\n",
    "    reference_dates = pd.DataFrame() \n",
    "    # Getting a column of dates from the start date -> yesterday \n",
    "    reference_dates['date'] = pd.date_range(start='2020-04-01', end=yesterday, freq='D') \n",
    "    # Merge the reference_dates dataframe into the data dataframe. Merges on the date column and uses outer since it will \n",
    "    # use a union of both df's dates. (All the dates are kept, duplicate dates are removed)\n",
    "    data = pd.merge(reference_dates, data, on=('date'), how='outer') \n",
    "    # If there is any missing data, then replace it with 0's.\n",
    "    data = data.fillna(0) \n",
    "    \n",
    "    \n",
    "    # Set the dataframe's index as the date column. This is required for sarimax model to work as it is a time-series\n",
    "    # forecasting algorithm.\n",
    "    data = data.set_index('date')\n",
    "    \n",
    "    # Since order and seasonal_order are based on origin state, destination state and travel month, these variables\n",
    "    # end up being the same for our entire set of grouped data.\n",
    "    # Unfortunately, the redshift database chose to store our tuple data as strings, so we need to first remove the\n",
    "    # brackets and extract the data, before repacking the data into a tuple.\n",
    "    # [1:-1] is used to remove the brackets. \n",
    "    order_str = (state_data['order'][0])[1:-1] \n",
    "    seasonal_order_str = (state_data['seasonal_order'][0])[1:-1] \n",
    "    # Create a tuple from the values. Restores the tuple data structure that we want.\n",
    "    order = tuple(map(int, order_str.split(', '))) \n",
    "    seasonal_order = tuple(map(int, seasonal_order_str.split(', '))) \n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Now that we have all of the data formatted and variables we need, we can configure and train the model\n",
    "        # and make our predictions.\n",
    "        \n",
    "        # Create the sarimax model and define the configuration.\n",
    "        mod_sarimax     = sm.tsa.SARIMAX(data, order=order, seasonal_order=seasonal_order) \n",
    "        # Fit/Train the sarimax model\n",
    "        smodel          = mod_sarimax.fit() \n",
    "        # Make predictions from today till month_end.\n",
    "        predict_results = smodel.get_prediction(start=today, end=month_end, freq='D', dynamic=True) \n",
    "                \n",
    "        # Most results are collected in the `summary_frame` attribute.\n",
    "        # Here we specify that we want a confidence level of 90%\n",
    "        predictions = predict_results.summary_frame(alpha=0.10) \n",
    "        \n",
    "        # Now that we have the predictions we want, we now have to format the resulting pandas dataframe into one \n",
    "        # that reflects the schema for returned dataframes.\n",
    "        \n",
    "        # Changes the index from date type into a string\n",
    "        predictions.index=predictions.index.strftime('%Y-%m-%d') \n",
    "        #Reset the index without creating a new index.\n",
    "        predictions.reset_index(inplace=True) \n",
    "        # Set the origin_state_code column to the origin_state\n",
    "        predictions['origin_state_code']=origin_state \n",
    "        # Set the destination_state_code column to the destination_state\n",
    "        predictions['destination_state_code']=destination_state \n",
    "        # Set the travel_month column to the month_end\n",
    "        predictions['travel_month']=month_end \n",
    "                \n",
    "        #Return pandas df.\n",
    "        return predictions \n",
    "    \n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        \n",
    "    # If an exception occurs during the code block in the try, create an empty dataframe to return with prediction \n",
    "    # values of 0.\n",
    "    empty = pd.DataFrame()    \n",
    "    empty['mean'] = 0\n",
    "    empty['mean_se'] = 0\n",
    "    empty['mean_ci_lower'] = 0\n",
    "    empty['mean_ci_upper'] = 0\n",
    "    empty['index']=month_end\n",
    "    empty['origin_state_code']=origin_state\n",
    "    empty['destination_state_code']=destination_state\n",
    "    empty['travel_month']=month_end\n",
    "    \n",
    "    return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our function setup, we can give spark some instructions to lazily execute. \n",
    "# However we firstly need to create tables that will be evaluated lazily (only executes instructions when a \n",
    "# value is needed/requested/output)\n",
    "\n",
    "# Creates a lazily evaluated view/table of the df dataframe.\n",
    "df.createOrReplaceTempView('data') \n",
    "# Creates a lazily evaluated view/table of the params_df dataframe.\n",
    "params_df.createOrReplaceTempView('params') \n",
    "\n",
    "# This long line of code uses spark sql to join the contents of the data view with the contents of the params view.\n",
    "# It joins the data together depending on whether the sales_date older than 2020-04-01 and whether there are\n",
    "# available parameters (no_order = 'N') or if arima is possible (no_arima = 'N')\n",
    "flattened = spark.sql(\"select data.sales_date, date_format(data.sales_date, 'MM/dd/yyyy') as date,  \" +\n",
    "                      \"data.origin_state_code, data.destination_state_code, data.month_start, data.month_end, data.total_searches, \" +\n",
    "                      \"params.dptr_month, params.m_order as order, params.m_seasonal_order as seasonal_order \" +\n",
    "                      \"from data \" +\n",
    "                      \"join params on params.origin_state_code = data.origin_state_code \" + \n",
    "                      \"and params.destination_state_code = data.destination_state_code \" +\n",
    "                      \"and params.dptr_month = CAST(EXTRACT(MONTH FROM data.month_start) AS bigint) \" +\n",
    "                      \"where data.sales_date >= '2020-04-01' \" +\n",
    "                      \"AND params.no_order = 'N' \" +\n",
    "                      \"AND params.no_arima = 'N' \" +\n",
    "                      \"ORDER BY sales_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the latest spark version at the current time, Spark 3.0.0. The apply function still works, however\n",
    "# is deprecated. If you are using this in version 3.0.0 or later, replace apply with applyInPandas.\n",
    "# In order for your Python function to be able to operate on the data,\n",
    "# PySpark converts the Spark DataFrame into a Pandas DataFrame before your function is called. \n",
    "# Likewise, when you return a Pandas DataFrame from your function, PySpark will convert it back into a Spark DataFrame.\n",
    "\n",
    "# Its worth noting, that PySpark will call your function for each group/partition of data that it has, hence, \n",
    "# in the example, there is a groupBy origin_state_code, destination_state_code, travel_month - so that the Python \n",
    "# function will operate on a Pandas DataFrame containing all the data for each combination.\n",
    "results = flattened.groupby('origin_state_code', 'destination_state_code','month_start').apply(run_sarima_bystate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now select whether we want to enforce partitioning or not.\n",
    "\n",
    "# Repartitions the results received into 4 separate partitions. (Use this to specify how many csv's you want to return.)\n",
    "\n",
    "# output = results.repartition(4)  #Commented out since we don't want to overwrite it.\n",
    "\n",
    "\n",
    "# Different version of the line above. Where partitioning isn't defined.\n",
    "output = results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase this number depending on what the highest sarimax- folder number is.\n",
    "count = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = count + 1\n",
    "# The data can now be written out to csv files.\n",
    "output.write.format(\"csv\").save(\"s3://folder path goes here/sarimax-\" + str(count) + \"/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
